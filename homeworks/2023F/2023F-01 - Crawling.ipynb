{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TYj8JqrtqkHZ"
   },
   "source": [
    "# 1. Crawler\n",
    "\n",
    "## 1.0. Related example\n",
    "\n",
    "This code shows `wget`-like tool written in python. Run it from console (`python wget.py`), make it work. Check the code, reuse, and modify for your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "nCQJWsL-qkHb",
    "ExecuteTime": {
     "end_time": "2023-09-06T19:17:27.149229513Z",
     "start_time": "2023-09-06T19:17:27.020600273Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-O FILENAME] url\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001B[0;31mSystemExit\u001B[0m\u001B[0;31m:\u001B[0m 2\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import hashlib\n",
    "\n",
    "\n",
    "def wget(url, filename):\n",
    "    # allow redirects - in case file is relocated\n",
    "    resp = requests.get(url, allow_redirects=True)\n",
    "    # this can also be 2xx, but for simplicity now we stick to 200\n",
    "    # you can also check for `resp.ok`\n",
    "    if resp.status_code != 200:\n",
    "        print(resp.status_code, resp.reason, 'for', url)\n",
    "        return\n",
    "\n",
    "    # just to be cool and print something\n",
    "    print(*[f\"{key}: {value}\" for key, value in resp.headers.items()], sep='\\n')\n",
    "    print()\n",
    "\n",
    "    # try to extract filename from url\n",
    "    if filename is None:\n",
    "        # start with http*, ends if ? or # appears (or none of)\n",
    "        m = re.search(\"^http.*/([^/\\?#]*)[\\?#]?\", url)\n",
    "        filename = m.group(1)\n",
    "        if not filename:\n",
    "            filename = hashlib.sha256(url.encode()).hexdigest()\n",
    "            # raise NameError(f\"Filename neither given, nor found for {url}\")\n",
    "\n",
    "    # what will you do in case 2 websites store file with the same name?\n",
    "    if os.path.exists(filename):\n",
    "        filename = hashlib.sha256(filename.encode()).hexdigest()\n",
    "        # raise OSError(f\"File {filename} already exists\")\n",
    "\n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(resp.content)\n",
    "        print(f\"File saved as {filename}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description='download file.')\n",
    "    parser.add_argument(\"-O\", type=str, default=None, dest='filename',\n",
    "                        help=\"output file name. Default -- taken from resource\")\n",
    "    parser.add_argument(\"url\", type=str, default=None, help=\"Provide URL here\")\n",
    "    args = parser.parse_args()\n",
    "    wget(args.url, args.filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5N5l9-0OqkHc"
   },
   "source": [
    "### 1.0.1. How to parse a page?\n",
    "\n",
    "If you build a crawler, you might follow one of the approaches:\n",
    "1. search for URLs in the page, assuming this is just a text.\n",
    "2. search for URLs in the places where URLs should appear: `<a href=..`, `<img src=...`, `<iframe src=...` and so on.\n",
    "\n",
    "To follow the first approach you can rely on some good regular expression. [Like this](https://stackoverflow.com/a/3809435).\n",
    "\n",
    "To follow the second approach just read one of these: [short answer](https://stackoverflow.com/questions/1080411/retrieve-links-from-web-page-using-python-and-beautifulsoup) or [exhaustive explanation](https://hackersandslackers.com/scraping-urls-with-beautifulsoup/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pvIby3tRqkHd"
   },
   "source": [
    "## 1.1. [10] Download and persist #\n",
    "Please complete a code for `load()`, `download()` and `persist()` methods of `Document` class. What they do:\n",
    "- for a given URL `download()` method downloads binary data and stores in `self.content`. It returns `True` for success, else `False`.\n",
    "- `persist()` method saves `self.content` somewhere in file system. We do it to avoid multiple downloads (for caching in other words).\n",
    "- `load()` method loads data from hard drive. Returns `True` for success.\n",
    "\n",
    "Tests checks that your code somehow works.\n",
    "\n",
    "**NB Passing the test doesn't mean you correctly completed the task.** These are **criteria, which have to be fullfilled**:\n",
    "1. URL is a unique identifier (as it is a subset of URI). Thus, documents with different URLs should be stored in different files. Typical errors: documents from the same domain are overwritten to the same file, URLs with similar endings are downloaded to the same file, etc.\n",
    "2. The document can be not only a text file, but also a binary. Pay attention that if you download `mp3` file, it still can be played. Hint: don't hurry to convert everything to text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "2n8-imQMqkHd",
    "ExecuteTime": {
     "end_time": "2023-09-06T19:20:52.328165805Z",
     "start_time": "2023-09-06T19:20:52.287589744Z"
    }
   },
   "outputs": [],
   "source": [
    "from os.path import exists\n",
    "import requests\n",
    "from urllib.parse import quote\n",
    "import hashlib\n",
    "\n",
    "\n",
    "class Document:\n",
    "\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.content = None\n",
    "\n",
    "    def get(self):\n",
    "        if not self.load():\n",
    "            if not self.download():\n",
    "                raise FileNotFoundError(self.url)\n",
    "            else:\n",
    "                self.persist()\n",
    "\n",
    "    def download(self):\n",
    "        #TODO download self.url content, store it in self.content and return True in case of success\n",
    "        response = requests.get(self.url, allow_redirects=True)\n",
    "        if response.status_code != requests.codes.ok:\n",
    "            return False\n",
    "        if response.content is None:\n",
    "            return False\n",
    "        self.content = response.content\n",
    "        return True\n",
    "\n",
    "    def persist(self):\n",
    "        #TODO write document content to hard drive\n",
    "        filename = hashlib.sha256(self.url.encode()).hexdigest()\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(self.content)\n",
    "\n",
    "    def load(self):\n",
    "        #TODO load content from hard drive, store it in self.content and return True in case of success\n",
    "        filename = hashlib.sha256(self.url.encode()).hexdigest()\n",
    "        if not exists(filename):\n",
    "            return False\n",
    "        with open(filename, 'rb') as f:\n",
    "            self.content = f.read()\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mI9zUoLQqkHd"
   },
   "source": [
    "### 1.1.1. Tests ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Loo2ayKKqkHd",
    "ExecuteTime": {
     "end_time": "2023-09-06T19:20:56.633870120Z",
     "start_time": "2023-09-06T19:20:56.613115831Z"
    }
   },
   "outputs": [],
   "source": [
    "doc = Document('https://github.com/YusufRoshdy/information-retrieval/raw/main/datasets/facts.txt')\n",
    "\n",
    "doc.get()\n",
    "assert doc.content, \"Document download failed\"\n",
    "assert \"You breathe on average about 8,409,600 times a year\" in str(doc.content), \"Document content error\"\n",
    "\n",
    "doc.get()\n",
    "assert doc.load(), \"Load should return true for saved document\"\n",
    "assert \"You breathe on average about 8,409,600 times a year\" in str(doc.content), \"Document load from disk error\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ef37KNL2qkHe"
   },
   "source": [
    "## 1.2. [10] Parse HTML\n",
    "`BeautifulSoap` library is a de facto standard to parse XML and HTML documents in python. Use it to complete `parse()` method that extracts document contents. You should initialize:\n",
    "1. `self.anchors` list of tuples `('text', 'url')` met in a document. Be aware, there exist relative links (e.g. `../content/pic.jpg`). Use `urllib.parse.urljoin()` to fix this issue.\n",
    "2. `self.images` list of images met in a document. Again, links can be relative to current page.\n",
    "3. `self.text` should keep plain text of the document without scripts, tags, comments and so on. You can refer to [this stackoverflow answer](https://stackoverflow.com/a/1983219) for details.\n",
    "\n",
    "**NB All these 3 criteria must be fulfilled to get full point for the task.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "U38KF0ZjqkHe",
    "ExecuteTime": {
     "end_time": "2023-09-06T19:21:04.039174036Z",
     "start_time": "2023-09-06T19:21:04.022543855Z"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "import urllib.parse\n",
    "\n",
    "\n",
    "class HtmlDocument(Document):\n",
    "\n",
    "    def parse(self):\n",
    "        #TODO extract plain text, images and links from the document\n",
    "        self.get()\n",
    "        soup = BeautifulSoup(self.content, 'html.parser')\n",
    "\n",
    "        links = soup.find_all('a', href=True)\n",
    "        self.anchors = [(a.text.strip(), urllib.parse.urljoin(self.url, a['href'].strip())) for a in links]\n",
    "\n",
    "        images = soup.find_all('img')\n",
    "        self.images = [urllib.parse.urljoin(self.url, img['src'].strip()) for img in images]\n",
    "\n",
    "        def tag_visible(element):\n",
    "            if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "                return False\n",
    "            if isinstance(element, Comment):\n",
    "                return False\n",
    "            return True\n",
    "\n",
    "        visible_texts = filter(tag_visible, soup.findAll(string=True))\n",
    "        self.text = \" \".join(t.strip() for t in visible_texts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XP5zdm3kqkHe"
   },
   "source": [
    "### 1.2.1. Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "EWjZSnU8qkHe",
    "ExecuteTime": {
     "end_time": "2023-09-06T19:21:07.268596808Z",
     "start_time": "2023-09-06T19:21:07.211083964Z"
    }
   },
   "outputs": [],
   "source": [
    "doc = HtmlDocument(\"https://innopolis.university/en/\")\n",
    "doc.get()\n",
    "doc.parse()\n",
    "\n",
    "assert \"Education, research and development\" in doc.text, \"Error parsing text\"\n",
    "assert \"https://innopolis.university/upload/resize_cache/iblock/e5c/510_340_2/deadline_extended.jpg\" in doc.images, \"Error parsing images\"\n",
    "assert any(p[1] == \"https://innopolis.university/en/faculty/\" for p in doc.anchors), \"Error parsing links\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Who0bIq0qkHe"
   },
   "source": [
    "## 1.3. [10] Document analysis ##\n",
    "Complete the code for `HtmlDocumentTextData` class. Implement word and sentence splitting (use any method you can propose).\n",
    "\n",
    "**Criteria to succeed in the task**:\n",
    "1. Your `get_word_stats()` method should return `Counter` object.\n",
    "2. Don't forget to lowercase your words for counting.\n",
    "3. Sentences should be obtained from inside `<body>` tag only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "XY2xa_e3qkHe",
    "ExecuteTime": {
     "end_time": "2023-09-06T19:21:11.995309978Z",
     "start_time": "2023-09-06T19:21:11.978639708Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/kamil/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# Make sure to download the Punkt tokenizer data\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "\n",
    "class HtmlDocumentTextData:\n",
    "\n",
    "    def __init__(self, url):\n",
    "        self.doc = HtmlDocument(url)\n",
    "        self.doc.get()\n",
    "        self.doc.parse()\n",
    "\n",
    "    def get_sentences(self):\n",
    "        #TODO implement sentence parser\n",
    "        soup = BeautifulSoup(self.doc.content, 'html.parser')\n",
    "        body = soup.find(\"body\")\n",
    "        if body:\n",
    "            text = body.get_text()\n",
    "            return sent_tokenize(text)\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    def get_word_stats(self):\n",
    "        #TODO return Counter object of the document, containing mapping {`word` -> count_in_doc}\n",
    "        soup = BeautifulSoup(self.doc.content, 'html.parser')\n",
    "        body = soup.find(\"body\")\n",
    "        if body:\n",
    "            text = body.get_text()\n",
    "            words = word_tokenize(text)\n",
    "            words = [word.lower() for word in words]\n",
    "            return Counter(words)\n",
    "        else:\n",
    "            return Counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EQ8a7n94qkHf"
   },
   "source": [
    "### 1.3.1. Tests ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "hz6aqgH7qkHf",
    "ExecuteTime": {
     "end_time": "2023-09-06T19:21:14.157485624Z",
     "start_time": "2023-09-06T19:21:14.064640447Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 33), ('and', 31), (',', 31), ('of', 31), ('university', 22), ('education', 15), ('in', 15), ('innopolis', 14), ('.', 13), ('research', 12)]\n"
     ]
    }
   ],
   "source": [
    "doc = HtmlDocumentTextData(\"https://innopolis.university/en\")\n",
    "\n",
    "print(doc.get_word_stats().most_common(10))\n",
    "assert [x for x in doc.get_word_stats().most_common(10) if x[0] == 'innopolis'], 'innopolis should be among most common'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FiQy0CkQtppS"
   },
   "source": [
    "## 1.4 [10] Account the caching policy\n",
    "Sometimes remote documents (especially when we speak about static content like `js` or `gif`) can swear that they will not change for some time. This is done by setting [Cache-Control response header](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cache-Control)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Rnbq9VoBtVNI",
    "ExecuteTime": {
     "end_time": "2023-09-06T19:21:17.379987716Z",
     "start_time": "2023-09-06T19:21:16.856320820Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'public, s-maxage=31536000, max-age=604800, stale-while-revalidate=604800, stale-if-error=604800, immutable'"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "requests.get('https://polyfill.io/v3/polyfill.min.js').headers['Cache-Control']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTJMOPu5uLkJ"
   },
   "source": [
    "Please study the documentation and implement a descendant to a Document class, which will refresh the document in case of expired cache even if the file is already on the hard drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "wqB-jm0jtx2C",
    "ExecuteTime": {
     "end_time": "2023-09-06T19:21:20.170644150Z",
     "start_time": "2023-09-06T19:21:20.147912593Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from math import ceil\n",
    "\n",
    "class CachedDocument(Document):\n",
    "    def __init__(self, url):\n",
    "        super().__init__(url)\n",
    "        self.expiration = 0\n",
    "    \n",
    "    def get(self):\n",
    "        if not self.load():\n",
    "            if not self.download():\n",
    "                raise FileNotFoundError(self.url)\n",
    "            else:\n",
    "                self.persist()\n",
    "        else:\n",
    "            current_time = ceil(time.time()) \n",
    "            if current_time >= self.expiration:\n",
    "                if not self.download():\n",
    "                    raise FileNotFoundError(self.url)\n",
    "                \n",
    "    def download(self):\n",
    "        response = requests.get(self.url, allow_redirects=True)\n",
    "        if response.status_code != requests.codes.ok:\n",
    "            return False\n",
    "        if response.content is None:\n",
    "            return False\n",
    "        self.content = response.content\n",
    "        cache_control = response.headers.get(\"Cache-Control\")\n",
    "        max_age = self.parse_max_age(cache_control)\n",
    "        self.expiration = ceil(time.time()) + max_age\n",
    "        return True\n",
    "    \n",
    "    @staticmethod\n",
    "    def parse_max_age(cache_control):\n",
    "        if cache_control is None:\n",
    "            return 0\n",
    "        parts = cache_control.split(\",\")\n",
    "        for part in parts:\n",
    "            if \"max-age\" in part:\n",
    "                max_age = int(part.split(\"=\")[1].strip())\n",
    "                return max_age\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P84vqtE9uQkJ"
   },
   "source": [
    "### 1.4.1. Tests ###\n",
    "\n",
    "Add logging to your code and show that your code behaves differently for documents with different caching policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "-_OwVQMEuOV4",
    "ExecuteTime": {
     "end_time": "2023-09-06T19:21:32.609537039Z",
     "start_time": "2023-09-06T19:21:22.719855284Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "doc = CachedDocument('https://polyfill.io/v3/polyfill.min.js')\n",
    "doc.get()\n",
    "time.sleep(2)\n",
    "doc.get()\n",
    "time.sleep(2)\n",
    "doc.get()\n",
    "\n",
    "doc = CachedDocument('https://yandex.ru/')\n",
    "doc.get()\n",
    "time.sleep(2)\n",
    "doc.get()\n",
    "time.sleep(2)\n",
    "doc.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z6C60rJOqkHf"
   },
   "source": [
    "## 1.5. [10] Crawling ##\n",
    "\n",
    "Method `crawl_generator()` is given starting url (`source`) and max depth of search. It should return a **generator** of `HtmlDocumentTextData` objects (return a document as soon as it is downloaded and parsed). You can benefit from `yield obj_name` python construction. Use `HtmlDocumentTextData.anchors` field to go deeper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "TLX_vNnSqkHf",
    "ExecuteTime": {
     "end_time": "2023-09-06T19:21:32.623133817Z",
     "start_time": "2023-09-06T19:21:32.617147469Z"
    }
   },
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "\n",
    "\n",
    "class Crawler:\n",
    "    def __init__(self):\n",
    "        self.visited = set()\n",
    "    \n",
    "    def _crawl_generator(self, source, depth=1):\n",
    "        if depth <=0:\n",
    "            return\n",
    "        try:\n",
    "            src_document = HtmlDocumentTextData(source)\n",
    "        except Exception:\n",
    "            return\n",
    "        self.visited.add(src_document.doc.url)\n",
    "        yield src_document\n",
    "        for anchor in src_document.doc.anchors:\n",
    "            if anchor in self.visited:\n",
    "                continue\n",
    "            self._crawl_generator(anchor, depth-1)\n",
    "\n",
    "    def crawl_generator(self, source, depth=1):\n",
    "        #TODO return real crawling results. Don't forget to process failures,\n",
    "        # exceptions, 3**, 4** codes\n",
    "        self.visited = set()\n",
    "        return self._crawl_generator(source, depth)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJ6k6m7XqkHf"
   },
   "source": [
    "### 1.5.1. Tests ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "d9zNWEvwqkHf",
    "ExecuteTime": {
     "end_time": "2023-09-06T19:21:32.716607422Z",
     "start_time": "2023-09-06T19:21:32.628696091Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://innopolis.university/en/\n",
      "350 distinct word(s) so far\n",
      "Done\n",
      "[('the', 33), ('and', 31), (',', 31), ('of', 31), ('university', 22), ('education', 15), ('in', 15), ('innopolis', 14), ('.', 13), ('research', 12), ('it', 11), ('international', 10), ('to', 10), ('for', 9), ('development', 8), ('a', 7), ('russian', 7), ('robotics', 6), ('you', 6), ('will', 6)]\n"
     ]
    }
   ],
   "source": [
    "crawler = Crawler()\n",
    "counter = Counter()\n",
    "\n",
    "for c in crawler.crawl_generator(\"https://innopolis.university/en/\", 2):\n",
    "    print(c.doc.url)\n",
    "    if c.doc.url[-4:] in ('.pdf', '.mp3', '.avi', '.mp4', '.txt'):\n",
    "        print(\"Skipping\", c.doc.url)\n",
    "        continue\n",
    "    counter.update(c.get_word_stats())\n",
    "    print(len(counter), \"distinct word(s) so far\")\n",
    "\n",
    "print(\"Done\")\n",
    "\n",
    "print(counter.most_common(20))\n",
    "assert [x for x in counter.most_common(20) if x[0] == 'innopolis'], 'innopolis sould be among most common'"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
