{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building inverted index and answering queries\n",
    "\n",
    "In this lab you are going to implement a standard document processing pipeline and then build a simple search engine based on it:\n",
    "- starting from crawling documents, \n",
    "- then building an inverted index,\n",
    "- and answering queries using this index.\n",
    "\n",
    "## Preprocessing\n",
    "\n",
    "First, we need a unified approach to documents and queries preprocessing. Implement a class responsible for that. Complete the code for given functions (most of them are just one-liners) and make sure you pass the tests. Make use of `nltk` library, `spacy`, or any other you know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T10:04:41.357778256Z",
     "start_time": "2023-09-25T10:04:40.988186238Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/kamil/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "class Preprocessor:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.stop_words = {'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by',\n",
    "                           'for', 'from', 'has', 'he', 'in', 'is', 'it', 'its',\n",
    "                           'of', 'on', 'that', 'the', 'to', 'was',\n",
    "                           'were', 'will', 'with'}\n",
    "        self.ps = nltk.stem.PorterStemmer()\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        #TODO word tokenize text using nltk lib\n",
    "        # YOUR CODE HERE\n",
    "        return nltk.word_tokenize(text)\n",
    "\n",
    "    def stem(self, word, stemmer):\n",
    "        #TODO stem word using provided stemmer\n",
    "        # YOUR CODE HERE\n",
    "        return stemmer.stem(word)\n",
    "\n",
    "    def is_apt_word(self, word):\n",
    "        #TODO check if word is appropriate - not a stop word and isalpha, \n",
    "        # i.e consists of letters, not punctuation, numbers, dates\n",
    "        # TODO YOUR CODE HERE\n",
    "        return word not in self.stop_words and word.isalpha()\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        tokenized = self.tokenize(text.lower())\n",
    "        return [self.stem(w, self.ps) for w in tokenized if self.is_apt_word(w)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T10:04:44.788739017Z",
     "start_time": "2023-09-25T10:04:44.780813060Z"
    }
   },
   "outputs": [],
   "source": [
    "prep = Preprocessor()\n",
    "text = 'To be, or not to be, that is the question'\n",
    "\n",
    "assert prep.tokenize(text) == ['To', 'be', ',', 'or', 'not', 'to', 'be',\n",
    "                               ',', 'that', 'is', 'the', 'question']\n",
    "assert prep.stem('retrieval', prep.ps) == 'retriev'\n",
    "assert prep.is_apt_word('qwerty123') is False\n",
    "assert prep.preprocess(text) == ['or', 'not', 'question']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawling and Indexing\n",
    "\n",
    "### Base classes\n",
    "\n",
    "Here are some base classes you will need for writing an indexer. The code is given, still, you need to change it slightly. Namely, the `parse()` method. Add the code which will extract relevant text from the documents, particularly trageting Reuters website structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T10:26:50.249459552Z",
     "start_time": "2023-09-25T10:26:50.211775089Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from urllib.parse import quote\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "import urllib.parse\n",
    "import os\n",
    "import hashlib\n",
    "\n",
    "\n",
    "class Document:\n",
    "\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "\n",
    "    def download(self):\n",
    "        try:\n",
    "            response = requests.get(self.url)\n",
    "            if response.status_code == 200:\n",
    "                self.content = response.content\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    def persist(self, path):\n",
    "        # this code is not supposed to be good :) \n",
    "        # Please discuss why this line is bad\n",
    "        url_hash = os.path.join(path, hashlib.sha256(quote(self.url).encode()).hexdigest())\n",
    "        with open(url_hash, 'wb') as f:\n",
    "            f.write(self.content)\n",
    "\n",
    "\n",
    "class HtmlReutersArticle(Document):\n",
    "\n",
    "    def normalize(self, href):\n",
    "        if href is not None and href[:4] != 'http':\n",
    "            href = urllib.parse.urljoin(self.url, href)\n",
    "        return href\n",
    "\n",
    "    def parse(self):\n",
    "\n",
    "        def tag_visible(element):\n",
    "            if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "                return False\n",
    "            if isinstance(element, Comment):\n",
    "                return False\n",
    "            return True\n",
    "\n",
    "        model = BeautifulSoup(self.content, \"html.parser\")\n",
    "\n",
    "        self.anchors = []\n",
    "        a = model.find_all('a')\n",
    "        for anchor in a:\n",
    "            href = self.normalize(anchor.get('href'))\n",
    "            text = anchor.text\n",
    "            self.anchors.append((text, href))\n",
    "\n",
    "        # extract only header and main content\n",
    "        # discuss why using classes like article-body__content__17Yit \n",
    "        # is the wrong strategy\n",
    "        header = model.find('h1') # TODO your code here\n",
    "        content = model.find('p') # TODO your code here\n",
    "\n",
    "        if header is None or content is None:\n",
    "            self.article_text = \"\"\n",
    "            return\n",
    "\n",
    "        texts = header.findAll(string=True) + content.findAll(string=True)\n",
    "        visible_texts = filter(tag_visible, texts)\n",
    "        self.article_text = \"\\n\".join(t.strip() for t in visible_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T10:26:52.523021947Z",
     "start_time": "2023-09-25T10:26:51.446586646Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'RPT Amazon launches virtual healthcare clinic in U.S. for common ailments\\nThe Amazon logo is seen outside its JFK8 distribution center in Staten Island, New York, U.S. November 25, 2020.  REUTERS/Brendan McDermid\\nAcquire Licensing Rights'"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = HtmlReutersArticle(\n",
    "    \"https://www.reuters.com/business/healthcare-pharmaceuticals/amazon-launches-virtual-healthcare-clinic-us-2022-11-15/\")\n",
    "doc.download()\n",
    "doc.parse()\n",
    "doc.persist(\"\")\n",
    "doc.article_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main class\n",
    "\n",
    "The main indexer logic is here. We organize it as a crawler generator that adds certain visited pages to inverted index and saves them on disk. \n",
    "\n",
    "- `crawl_generator_for_index` method crawles the given website doing BFS, starting from `source` within given `depth`. Considers only inner pages (starting with https://www.reuters.com/...) for visiting. To speed up, do not consider pages with content type other than `html`: `.pdf`, `.mp3`, `.avi`, `.mp4`, `.txt`, ... . If crawler encounters an article page (of a form https://www.reuters.com/TOPIC/NAME...), it saves its content in a file in `collection_path` folder and populates the inverted index calling `index_doc` method. When done, saves on disk three resulting dictionaries:\n",
    "    - `doc_urls`: `{doc_id : url}`\n",
    "    - `index`: `{term : [collection_frequency, (doc_id_1, doc_freq_1), (doc_id_2, doc_freq_2), ...]}`\n",
    "    - `doc_lengths`: `{doc_id : doc_length}` \n",
    "\n",
    "    `limit` parameter is given for testing - if not `None`, break the loop when number of saved articles exceeds the `limit` and return without writing dictionaries to disk.\n",
    "    \n",
    "- `index_doc` method parses and preprocesses the content of a `doc` and adds it to the inverted index. Also keeps track of document lengths in a `doc_lengths` dictionary.\n",
    "\n",
    "\n",
    "**TODO**: at line 38 we have a regular expression that matches for https://www.reuters.com/TOPIC/ARTICLE-NAME links, but fails for topic pages and external websites. Please write this regular expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T10:53:14.472323981Z",
     "start_time": "2023-09-25T10:53:14.422451218Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from queue import Queue\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "class ReutersSpecificIndexer:\n",
    "\n",
    "    def __init__(self):\n",
    "        # dictionaries to populate\n",
    "        self.doc_urls = {}\n",
    "        self.index = {}\n",
    "        self.doc_lengths = {}\n",
    "        # preprocessor\n",
    "        self.prep = Preprocessor()\n",
    "\n",
    "    def crawl_generator_for_index(self, source, depth, collection_path=\"collection\", limit=None):\n",
    "        #TODO generate url-s for visiting\n",
    "\n",
    "        q = Queue()\n",
    "        q.put((source, 0))\n",
    "        visited = set()\n",
    "        doc_counter = 0\n",
    "        # creating a folder if needed\n",
    "        if not os.path.exists(collection_path):\n",
    "            os.makedirs(collection_path)\n",
    "        # doing a BFS\n",
    "        while not q.empty():\n",
    "            url, url_depth = q.get()\n",
    "            if url not in visited:\n",
    "                visited.add(url)\n",
    "                try:\n",
    "                    doc = HtmlReutersArticle(url)  # download and parse url\n",
    "                    if doc.download():\n",
    "                        doc.parse()\n",
    "                        # TODO PLEASE WRITE A CORRECT REGULAR EXPRESSION\n",
    "                        if re.match(r'https://www.reuters.com/((\\w+/?)*)', url):\n",
    "                            doc.persist(collection_path)\n",
    "                            self.doc_urls[doc_counter] = url\n",
    "                            self.index_doc(doc, doc_counter)\n",
    "                            doc_counter += 1\n",
    "                            yield doc\n",
    "                            if limit is not None and doc_counter == limit:\n",
    "                                return\n",
    "\n",
    "                            # filter links, consider only inner html pages\n",
    "                        if url_depth + 1 < depth:\n",
    "                            valid_anchors = filter(self.accepted_url, doc.anchors)\n",
    "                            for a in valid_anchors:\n",
    "                                q.put((a[1], url_depth + 1))\n",
    "\n",
    "                except FileNotFoundError as e:\n",
    "                    print(\"Analyzing\", url, \"led to FileNotFoundError\")\n",
    "\n",
    "    def accepted_url(self, anchor):\n",
    "        url = str(anchor[1])\n",
    "        if not url.startswith(\"https://www.reuters.com/\"):\n",
    "            return False\n",
    "        if url[-4:] in ('.pdf', '.mp3', '.avi', '.mp4', '.txt'):\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def index_doc(self, doc, doc_id):\n",
    "        # add documents to index\n",
    "        doc.parse()\n",
    "        # preprocess - tokenize, remove stopwords and non-alphanumeric tokens and stem\n",
    "        content = self.prep.preprocess(doc.article_text)\n",
    "        self.doc_lengths[doc_id] = len(content)\n",
    "        # get dict of terms in current article\n",
    "        article_index = Counter(content)\n",
    "        # update global index\n",
    "        for term in article_index.keys():\n",
    "            article_freq = article_index[term]\n",
    "            if term not in self.index:\n",
    "                self.index[term] = [article_freq, (doc_id, article_freq)]\n",
    "            else:\n",
    "                self.index[term][0] += article_freq\n",
    "                self.index[term].append((doc_id, article_freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests\n",
    "\n",
    "Please make sure your crawler prints out urls with `print(k, c.url)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T10:53:42.356935175Z",
     "start_time": "2023-09-25T10:53:16.483239044Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 https://www.reuters.com/technology\n",
      "2 https://www.reuters.com/technology#main-content\n",
      "3 https://www.reuters.com/differentiator/\n",
      "4 https://www.reuters.com/\n",
      "5 https://www.reuters.com/world/\n",
      "6 https://www.reuters.com/business/\n",
      "7 https://www.reuters.com/markets/\n",
      "8 https://www.reuters.com/sustainability/\n",
      "9 https://www.reuters.com/legal/\n",
      "10 https://www.reuters.com/breakingviews/\n",
      "11 https://www.reuters.com/technology/\n",
      "12 https://www.reuters.com/investigates/\n",
      "13 https://www.reuters.com/account/register/sign-up/&journeyStart=navigation\n",
      "14 https://www.reuters.com/technology/sec-collects-wall-streets-private-messages-whatsapp-probe-escalates-sources-2023-09-25/\n",
      "15 https://www.reuters.com/tags/securities-enforcement/\n"
     ]
    }
   ],
   "source": [
    "indexer = ReutersSpecificIndexer()\n",
    "k = 1\n",
    "for c in indexer.crawl_generator_for_index(\n",
    "        source=\"https://www.reuters.com/technology\",\n",
    "        depth=2,\n",
    "        collection_path=\"test_collection\",\n",
    "        limit=15):\n",
    "    print(k, c.url)\n",
    "    k += 1\n",
    "\n",
    "assert type(indexer.index) is dict\n",
    "assert type(indexer.index['reuter']) is list\n",
    "assert type(indexer.index['reuter'][0]) is int\n",
    "assert type(indexer.index['reuter'][1]) is tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please test these documents contain a desired stem (or its derivate):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T10:56:33.978163651Z",
     "start_time": "2023-09-25T10:56:33.971649345Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, (5, 1)]\n",
      "https://www.reuters.com/business/\n"
     ]
    }
   ],
   "source": [
    "some_stem = 'bank'\n",
    "print(indexer.index[some_stem])\n",
    "for pair in indexer.index[some_stem][1:]:\n",
    "    print(indexer.doc_urls[pair[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4. Building an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T11:02:32.976268534Z",
     "start_time": "2023-09-25T10:59:46.823709509Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 https://www.reuters.com/\n",
      "2 https://www.reuters.com/#main-content\n",
      "3 https://www.reuters.com/differentiator/\n",
      "4 https://www.reuters.com/world/\n",
      "5 https://www.reuters.com/business/\n",
      "6 https://www.reuters.com/markets/\n",
      "7 https://www.reuters.com/sustainability/\n",
      "8 https://www.reuters.com/legal/\n",
      "9 https://www.reuters.com/breakingviews/\n",
      "10 https://www.reuters.com/technology/\n",
      "11 https://www.reuters.com/investigates/\n",
      "12 https://www.reuters.com/account/register/sign-up/&journeyStart=navigation\n",
      "13 https://www.reuters.com/markets/quote/.SPX/\n",
      "14 https://www.reuters.com/markets/quote/.IXIC/\n",
      "15 https://www.reuters.com/markets/quote/.DJI/\n",
      "16 https://www.reuters.com/markets/quote/.STOXX/\n",
      "17 https://www.reuters.com/markets/quote/.FTSE/\n",
      "18 https://www.reuters.com/markets/quote/.N225/\n",
      "19 https://www.reuters.com/lseg/\n",
      "20 https://www.reuters.com/technology/sec-collects-wall-streets-private-messages-whatsapp-probe-escalates-sources-2023-09-25/\n",
      "21 https://www.reuters.com/markets/asia/china-bans-nomura-senior-investment-banker-leaving-mainland-sources-2023-09-25/\n",
      "22 https://www.reuters.com/markets/commodities/inside-vietnams-plans-dent-chinas-rare-earths-dominance-2023-09-25/\n",
      "23 https://www.reuters.com/markets/macromatters/\n",
      "24 https://www.reuters.com/markets/us/why-traders-arent-buying-feds-higher-for-longer-vision-2023-09-25/\n",
      "25 https://www.reuters.com/markets/deals/\n",
      "26 https://www.reuters.com/markets/deals/amazon-steps-up-ai-race-with-up-4-billion-deal-invest-anthropic-2023-09-25/\n",
      "27 https://www.reuters.com/world/uk/slump-uk-retail-sales-eases-september-cbi-says-2023-09-25/\n",
      "28 https://www.reuters.com/world/uk/britain-proposes-new-diversity-rules-financial-firms-2023-09-25/\n",
      "29 https://www.reuters.com/world/us/how-desantis-early-missteps-hobbled-his-us-presidential-bid-2023-09-25/\n",
      "30 https://www.reuters.com/world/asia-pacific/turkeys-erdogan-meet-azeris-aliyev-thousands-flee-karabakh-2023-09-25/\n",
      "31 https://www.reuters.com/world/asia-pacific/senior-us-officials-traveling-armenia-karabakhs-armenians-start-leave-2023-09-25/\n",
      "32 https://www.reuters.com/world/fleeing-bombs-death-karabakh-armenians-recount-visceral-fear-hunger-2023-09-24/\n",
      "33 https://www.reuters.com/markets/us/global-markets-view-usa-2023-09-25/\n",
      "34 https://www.reuters.com/world/us/writers-reach-tentative-labor-agreement-with-hollywood-studios-2023-09-25/\n",
      "35 https://www.reuters.com/world/us/\n",
      "36 https://www.reuters.com/sustainability/climate-energy/after-un-meeting-countries-brace-cop28-fossil-fuel-fight-2023-09-25/\n",
      "37 https://www.reuters.com/world/taliban-weighs-using-us-mass-surveillance-plan-met-with-chinas-huawei-2023-09-25/\n",
      "38 https://www.reuters.com/world/china/evergrande-shares-sink-after-saying-it-is-unable-issue-new-debt-2023-09-25/\n",
      "39 https://www.reuters.com/world/europe/\n",
      "40 https://www.reuters.com/world/europe/kosovo-police-enter-northern-village-after-shootout-with-gunmen-killed-four-2023-09-25/\n",
      "41 https://www.reuters.com/world/europe/jailed-italian-mafia-boss-messina-denaro-dies-afp-citing-italian-media-2023-09-25/\n",
      "42 https://www.reuters.com/world/europe/slovak-election-hopeful-fico-illusory-deal-with-ukraines-eu-membership-now-2023-09-25/\n",
      "43 https://www.reuters.com/world/europe/oscar-winning-italian-actress-sophia-loren-hospital-after-fall-spokesman-2023-09-25/\n",
      "44 https://www.reuters.com/world/asia-pacific/\n",
      "45 https://www.reuters.com/world/asia-pacific/polls-show-australia-indigenous-referendum-support-slipping-likely-fail-2023-09-24/\n",
      "46 https://www.reuters.com/world/asia-pacific/north-korea-says-cooperation-with-russia-natural-neighbours-kcna-2023-09-24/\n",
      "47 https://www.reuters.com/world/asia-pacific/chinese-tourists-get-vip-welcome-thai-visa-waiver-scheme-begins-2023-09-25/\n",
      "48 https://www.reuters.com/world/asia-pacific/poll-shows-new-zealand-populist-party-could-be-kingmaker-labour-slides-2023-09-25/\n",
      "49 https://www.reuters.com/world/us/us-house-press-forward-with-spending-cuts-despite-shutdown-risk-2023-09-25/\n",
      "50 https://www.reuters.com/world/us/republicans-appeal-far-right-conservatives-avert-us-government-shutdown-2023-09-24/\n",
      "51 https://www.reuters.com/world/us/california-escapes-fire-season-mostly-unharmed-danger-could-lie-ahead-2023-09-25/\n",
      "52 https://www.reuters.com/world/us/weakening-ophelia-brings-more-rain-flood-risk-us-east-coast-2023-09-24/\n",
      "53 https://www.reuters.com/sports/\n",
      "54 https://www.reuters.com/sports/tennis/americans-tiafoe-shelton-secure-laver-cup-title-team-world-2023-09-24/\n",
      "55 https://www.reuters.com/sports/golf/pettersen-hails-europes-solheim-legends-after-retaining-cup-2023-09-25/\n",
      "56 https://www.reuters.com/sports/mlb-roundup-lowly-royals-finish-3-game-sweep-astros-2023-09-25/\n",
      "57 https://www.reuters.com/sports/north-korean-shooters-snub-south-koreans-podium-2023-09-25/\n",
      "58 https://www.reuters.com/pictures/\n",
      "59 https://www.reuters.com/world/best-us-open-2023-09-11/\n",
      "60 https://www.reuters.com/world/deadly-earthquake-rocks-morocco-2023-09-11/\n",
      "61 https://www.reuters.com/world/defining-images-911-attacks-2023-09-11/\n",
      "62 https://www.reuters.com/lifestyle/rb-star-usher-headline-super-bowl-58-halftime-show-2023-09-24/\n",
      "63 https://www.reuters.com/lifestyle/\n",
      "64 https://www.reuters.com/lifestyle/giorgio-armani-offers-soft-fluid-looks-milan-fashion-week-2023-09-24/\n",
      "65 https://www.reuters.com/lifestyle/all-black-white-looks-rule-dolce-gabbana-catwalk-milan-2023-09-23/\n",
      "66 https://www.reuters.com/world/americas/mexican-police-cuff-crooked-demon-doll-chucky-2023-09-23/\n",
      "67 https://www.reuters.com/science/\n",
      "68 https://www.reuters.com/fact-check/\n",
      "69 https://www.reuters.com/tools/mobile/us\n",
      "70 https://www.reuters.com/info-pages/advertising-guidelines/\n",
      "71 https://www.reuters.com/coupons/\n",
      "72 https://www.reuters.com/info-pages/terms-of-use/\n",
      "73 https://www.reuters.com/info-pages/contact-us/\n",
      "74 https://www.reuters.com/info-pages/disclaimer/\n",
      "75 https://www.reuters.com/world/#main-content\n",
      "76 https://www.reuters.com/world/asia-pacific/philippines-remove-barrier-placed-by-china-south-china-sea-national-security-2023-09-25/\n",
      "77 https://www.reuters.com/world/canada-house-speaker-apologizes-recognition-veteran-who-fought-nazis-2023-09-24/\n",
      "78 https://www.reuters.com/world/punjabs-sikhs-fear-canada-india-row-threatens-them-home-abroad-2023-09-25/\n",
      "79 https://www.reuters.com/science/nasas-first-asteroid-sample-parachutes-into-utah-desert-2023-09-24/\n",
      "80 https://www.reuters.com/world/armenia-calls-un-mission-monitor-rights-nagorno-karabakh-2023-09-24/\n",
      "81 https://www.reuters.com/world/asia-pacific/philippines-condemns-chinese-floating-barrier-south-china-sea-2023-09-24/\n",
      "82 https://www.reuters.com/world/europe/russias-air-attack-odesa-injures-one-damages-infrastructure-ukraine-official-2023-09-25/\n",
      "83 https://www.reuters.com/world/asia-pacific/un-worried-about-vietnam-arrest-energy-expert-after-bidens-visit-2023-09-25/\n",
      "84 https://www.reuters.com/tags/human-rights/\n",
      "85 https://www.reuters.com/world/china-could-do-a-lot-reduce-eu-perception-risk-eu-trade-chief-2023-09-25/\n",
      "86 https://www.reuters.com/business/#main-content\n",
      "87 https://www.reuters.com/technology/chinas-huawei-kicks-off-product-launch-event-by-thanking-country-its-support-2023-09-25/\n",
      "88 https://www.reuters.com/sustainability/lego-abandons-effort-make-oil-free-bricks-ft-2023-09-24/\n",
      "89 https://www.reuters.com/business/autos-transportation/ford-says-significant-gaps-remain-reach-uaw-contract-deal-2023-09-25/\n",
      "90 https://www.reuters.com/business/world-at-work/\n",
      "91 https://www.reuters.com/business/retail-consumer/us-sanctions-authority-delayed-kfcs-russia-exit-franchise-owner-2023-09-25/\n",
      "92 https://www.reuters.com/business/retail-consumer/\n",
      "93 https://www.reuters.com/markets/global-markets-wrapup-1-pix-2023-09-25/\n",
      "94 https://www.reuters.com/business/energy/oil-climbs-with-tight-supply-back-focus-2023-09-25/\n",
      "95 https://www.reuters.com/tags/transport-fuels/\n",
      "96 https://www.reuters.com/markets/europe/germany-scraps-plans-more-stringent-building-standards-prop-up-industry-2023-09-24/\n",
      "97 https://www.reuters.com/business/autos-transportation/uks-fossil-fuel-car-ban-delay-may-only-stall-investment-2023-09-25/\n",
      "98 https://www.reuters.com/business/charged/\n",
      "99 https://www.reuters.com/technology/openai-ceo-says-possible-get-regulation-wrong-should-not-fear-it-2023-09-25/\n",
      "100 https://www.reuters.com/markets/deals/aviva-agrees-buy-aigs-uk-protection-business-563-million-2023-09-25/\n"
     ]
    }
   ],
   "source": [
    "indexer = ReutersSpecificIndexer()\n",
    "for k, c in enumerate(\n",
    "        indexer\n",
    "                .crawl_generator_for_index(\n",
    "            \"https://www.reuters.com/\",\n",
    "            3,\n",
    "            \"docs_collection\",\n",
    "            limit=100  # optional limit\n",
    "        )):\n",
    "    print(k + 1, c.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index statistics\n",
    "\n",
    "Load the index and print the statistics. May you use this data to update stopwords list?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T11:02:50.762255425Z",
     "start_time": "2023-09-25T11:02:50.717877164Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total index keys count 1129\n",
      "\n",
      "Top stems by number of documents they apperared in:\n",
      "[('right', 51), ('licens', 51), ('acquir', 50), ('septemb', 27), ('photo', 27), ('reuter', 24), ('after', 21), ('trade', 13), ('peopl', 13), ('china', 13), ('via', 12), ('news', 11), ('lower', 11), ('world', 10), ('more', 10), ('monday', 10), ('financi', 10), ('up', 9), ('nation', 9), ('dure', 9)]\n",
      "\n",
      "Top stems by overall frequency:\n",
      "[('right', 54), ('licens', 50), ('acquir', 49), ('reuter', 31), ('photo', 28), ('septemb', 27), ('after', 21), ('china', 20), ('news', 16), ('new', 16), ('world', 13), ('trade', 13), ('peopl', 12), ('via', 11), ('financi', 11), ('south', 10), ('provid', 10), ('nation', 10), ('lower', 10), ('week', 9)]\n"
     ]
    }
   ],
   "source": [
    "print('Total index keys count', len(indexer.index))\n",
    "\n",
    "print('\\nTop stems by number of documents they apperared in:')\n",
    "sorted_by_n_docs = sorted(indexer.index.items(), key=lambda kv: (len(kv[1]), kv[0]), reverse=True)\n",
    "print([(sorted_by_n_docs[i][0], len(sorted_by_n_docs[i][1])) for i in range(20)])\n",
    "\n",
    "print('\\nTop stems by overall frequency:')\n",
    "sorted_by_freq = sorted(indexer.index.items(), key=lambda kv: (kv[1][0], kv[0]), reverse=True)\n",
    "print([(sorted_by_freq[i][0], sorted_by_freq[i][1][0]) for i in range(20)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answering a query (finally)\n",
    "\n",
    "Now, given that we already have built the inverted index, it's time to utilize it for answering user queries. In this class there are two methods you need to implement:\n",
    "- `boolean_retrieval`, the simplest form of document retrieval which returns a set of documents such that each one contains all query terms. Returns a set of document ids. Refer to *ch.1* of the book for details;\n",
    "- `okapi_scoring`, Okapi BM25 ranking function - assigns scores to documents in the collection that are relevant to the user query. Returns a dictionary of scores, `doc_id:score`. Read about it in [Wikipedia](https://en.wikipedia.org/wiki/Okapi_BM25#The_ranking_function) and implement accordingly.\n",
    "\n",
    "Both methods accept `query` parameter in a form of a dictionary, `term:frequency`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T11:20:39.937972Z",
     "start_time": "2023-09-25T11:20:39.923557794Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import functools\n",
    "import math\n",
    "\n",
    "\n",
    "class QueryProcessing:\n",
    "\n",
    "    @staticmethod\n",
    "    def prepare_query(raw_query):\n",
    "        # Initialize the preprocessor\n",
    "        prep = Preprocessor()\n",
    "\n",
    "        # The query should be treated similarly to the documents, so we preprocess it\n",
    "        query = prep.preprocess(raw_query)\n",
    "\n",
    "        # Once preprocessed, we count how many times each term appears in the query\n",
    "        # This gives us a dictionary where keys are terms and values are their frequencies\n",
    "        return Counter(query)\n",
    "\n",
    "    @staticmethod\n",
    "    def boolean_retrieval(query, index):\n",
    "        # This function retrieves documents that contain ALL query terms\n",
    "\n",
    "        # Start by creating an empty list to hold postings for each term in the query\n",
    "        postings = []\n",
    "\n",
    "        # For each term in the query\n",
    "        for term in query.keys():\n",
    "            # If the term isn't in the index, we skip it\n",
    "            if term not in index.keys():\n",
    "                continue\n",
    "            # Get postings for the term from the index\n",
    "            term_postings = index[term]\n",
    "            # Extract only document IDs from the postings\n",
    "            term_postings = [posting[0] for posting in term_postings[1:]]\n",
    "            # Add the list of document IDs to our postings list\n",
    "            postings.append(term_postings)\n",
    "        # We want documents that contain ALL query terms, so we find the intersection of all postings\n",
    "        return functools.reduce(lambda x, y: set(x).intersection(set(y)), postings)\n",
    "        # return {0, 2, 3}\n",
    "\n",
    "    @staticmethod\n",
    "    def okapi_scoring(query, doc_lengths, index, k1=1.2, b=0.75):\n",
    "        # This function computes a score for each document based on its relevance to the query\n",
    "\n",
    "        # Initialize an empty dictionary to hold scores for each document\n",
    "        scores = []\n",
    "\n",
    "        # Total number of documents in our collection\n",
    "        doc_num = len(doc_lengths)\n",
    "        # Compute the average document length\n",
    "        avgdl = sum(doc_lengths) / doc_num\n",
    "\n",
    "        # For each term in the query\n",
    "        for term in query.keys():\n",
    "            # If the term isn't in the index, we skip it\n",
    "            if term not in index.keys():\n",
    "                continue\n",
    "\n",
    "            # Compute inverse document frequency (IDF) for the term\n",
    "            idf = math.log((doc_num - index[term][0] + 0.5) / (index[term][0] + 0.5) + 1)\n",
    "            # Get postings for the term\n",
    "            \n",
    "            # For each posting, compute a score for the associated document\n",
    "            for posting in postings:\n",
    "                ...\n",
    "\n",
    "                # If the document already has a score, add the new score to it. Otherwise, set the new score\n",
    "\n",
    "        return scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_doc_lengths = {1: 20, 2: 15, 3: 10, 4: 20, 5: 30}\n",
    "test_index = {'x': [2, (1, 1), (2, 1)], 'y': [2, (1, 1), (3, 1)], 'z': [3, (2, 1), (4, 2)]}\n",
    "\n",
    "test_query1 = QueryProcessing.prepare_query('x z')\n",
    "test_query2 = QueryProcessing.prepare_query('x y')\n",
    "\n",
    "assert QueryProcessing.boolean_retrieval(test_query1, test_index) == {2}\n",
    "assert QueryProcessing.boolean_retrieval(test_query2, test_index) == {1}\n",
    "okapi_res = QueryProcessing.okapi_scoring(test_query2, test_doc_lengths, test_index)\n",
    "assert all(k in okapi_res for k in (1, 2, 3))\n",
    "assert not any(k in okapi_res for k in (4, 5))\n",
    "assert okapi_res[1] > okapi_res[3] > okapi_res[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And now query the real index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "russia ukraine\n",
      "\t https://www.reuters.com/world/us/republicans-appeal-far-right-conservatives-avert-us-government-shutdown-2023-09-24/\n",
      "\t == Okapi Time: 0.00002 ==\n",
      "\t https://www.reuters.com/world/us/republicans-appeal-far-right-conservatives-avert-us-government-shutdown-2023-09-24/ 3.036059120543364\n",
      "\t https://www.reuters.com/world/armenian-pm-says-likelihood-rising-that-ethnic-armenians-will-leave-karabakh-2023-09-24/ 2.025785179024357\n",
      "\t https://www.reuters.com/world/ukraines-zelenskiy-says-he-met-top-businessmen-during-us-visit-2023-09-24/ 1.8732517929970098\n",
      "\t https://www.reuters.com/world/defining-images-911-attacks-2023-09-11/ 1.6544604389625321\n",
      "\t https://www.reuters.com/world/europe/russian-installed-head-donetsk-imposes-5-hour-curfew-2023-09-24/ 1.3769625675282013\n",
      "\n",
      "Hong Kong\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unbound method set.intersection() needs an argument",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m/home/kamil/Desktop/New IR/information-retrieval/labs/lab-04/04 - Inverted index.ipynb Cell 23\u001B[0m line \u001B[0;36m7\n\u001B[1;32m      <a href='vscode-notebook-cell:/home/kamil/Desktop/New%20IR/information-retrieval/labs/lab-04/04%20-%20Inverted%20index.ipynb#X31sZmlsZQ%3D%3D?line=4'>5</a>\u001B[0m \u001B[39mprint\u001B[39m(q)\n\u001B[1;32m      <a href='vscode-notebook-cell:/home/kamil/Desktop/New%20IR/information-retrieval/labs/lab-04/04%20-%20Inverted%20index.ipynb#X31sZmlsZQ%3D%3D?line=5'>6</a>\u001B[0m qobj \u001B[39m=\u001B[39m QueryProcessing\u001B[39m.\u001B[39mprepare_query(q)\n\u001B[0;32m----> <a href='vscode-notebook-cell:/home/kamil/Desktop/New%20IR/information-retrieval/labs/lab-04/04%20-%20Inverted%20index.ipynb#X31sZmlsZQ%3D%3D?line=6'>7</a>\u001B[0m \u001B[39mfor\u001B[39;00m res \u001B[39min\u001B[39;00m QueryProcessing\u001B[39m.\u001B[39;49mboolean_retrieval(qobj, indexer\u001B[39m.\u001B[39;49mindex):\n\u001B[1;32m      <a href='vscode-notebook-cell:/home/kamil/Desktop/New%20IR/information-retrieval/labs/lab-04/04%20-%20Inverted%20index.ipynb#X31sZmlsZQ%3D%3D?line=7'>8</a>\u001B[0m     \u001B[39mprint\u001B[39m(\u001B[39m'\u001B[39m\u001B[39m\\t\u001B[39;00m\u001B[39m'\u001B[39m, indexer\u001B[39m.\u001B[39mdoc_urls[res])\n\u001B[1;32m     <a href='vscode-notebook-cell:/home/kamil/Desktop/New%20IR/information-retrieval/labs/lab-04/04%20-%20Inverted%20index.ipynb#X31sZmlsZQ%3D%3D?line=9'>10</a>\u001B[0m s \u001B[39m=\u001B[39m time\u001B[39m.\u001B[39mtime()\n",
      "\u001B[1;32m/home/kamil/Desktop/New IR/information-retrieval/labs/lab-04/04 - Inverted index.ipynb Cell 23\u001B[0m line \u001B[0;36m2\n\u001B[1;32m     <a href='vscode-notebook-cell:/home/kamil/Desktop/New%20IR/information-retrieval/labs/lab-04/04%20-%20Inverted%20index.ipynb#X31sZmlsZQ%3D%3D?line=24'>25</a>\u001B[0m     posting \u001B[39m=\u001B[39m [i[\u001B[39m0\u001B[39m] \u001B[39mfor\u001B[39;00m i \u001B[39min\u001B[39;00m posting]\n\u001B[1;32m     <a href='vscode-notebook-cell:/home/kamil/Desktop/New%20IR/information-retrieval/labs/lab-04/04%20-%20Inverted%20index.ipynb#X31sZmlsZQ%3D%3D?line=25'>26</a>\u001B[0m     postings\u001B[39m.\u001B[39mappend(posting)\n\u001B[0;32m---> <a href='vscode-notebook-cell:/home/kamil/Desktop/New%20IR/information-retrieval/labs/lab-04/04%20-%20Inverted%20index.ipynb#X31sZmlsZQ%3D%3D?line=26'>27</a>\u001B[0m docs \u001B[39m=\u001B[39m \u001B[39mset\u001B[39;49m\u001B[39m.\u001B[39;49mintersection(\u001B[39m*\u001B[39;49m\u001B[39mmap\u001B[39;49m(\u001B[39mset\u001B[39;49m,postings))\n\u001B[1;32m     <a href='vscode-notebook-cell:/home/kamil/Desktop/New%20IR/information-retrieval/labs/lab-04/04%20-%20Inverted%20index.ipynb#X31sZmlsZQ%3D%3D?line=27'>28</a>\u001B[0m \u001B[39mreturn\u001B[39;00m docs \n\u001B[1;32m     <a href='vscode-notebook-cell:/home/kamil/Desktop/New%20IR/information-retrieval/labs/lab-04/04%20-%20Inverted%20index.ipynb#X31sZmlsZQ%3D%3D?line=29'>30</a>\u001B[0m \u001B[39mreturn\u001B[39;00m {\u001B[39m0\u001B[39m, \u001B[39m1\u001B[39m, \u001B[39m3\u001B[39m}\n",
      "\u001B[0;31mTypeError\u001B[0m: unbound method set.intersection() needs an argument"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "queries = [\"russia ukraine\", \"Hong Kong\", \"crypto money\"]\n",
    "for q in queries:\n",
    "    print(q)\n",
    "    qobj = QueryProcessing.prepare_query(q)\n",
    "    for res in QueryProcessing.boolean_retrieval(qobj, indexer.index):\n",
    "        print('\\t', indexer.doc_urls[res])\n",
    "\n",
    "    s = time.time()\n",
    "    okapi_res = QueryProcessing.okapi_scoring(qobj, indexer.doc_lengths, indexer.index)\n",
    "    e = time.time()\n",
    "    print(f\"\\t == Okapi Time: {e - s:.5f} ==\")\n",
    "    for res in okapi_res:\n",
    "        print('\\t', indexer.doc_urls[res], okapi_res[res])\n",
    "\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
